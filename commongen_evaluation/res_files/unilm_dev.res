/home/bill/CommonGen-plus/dataset/final_data/commongen/commongen.dev.src_alpha.txt
/home/bill/CommonGen-plus/dataset/final_data/commongen/commongen.dev.tgt.txt
/home/bill/CommonGen-plus/methods/unilm_based/decoded_sentences/dev/model.10.bin.dev
MLM-Scores
Evaluating with bert-base-en-uncased
score with bert-base-en-uncased is -18.6740003803104
Evaluating with roberta-base-en-cased
score with roberta-base-en-cased is -21.661700190125895
Evaluating with gpt2-345m-en-cased
score with gpt2-345m-en-cased is -46.0381824214284
