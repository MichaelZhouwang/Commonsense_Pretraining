/home/bill/CommonGen-plus/dataset/final_data/commongen/commongen.test.src_alpha.txt
/home/bill/CommonGen-plus/dataset/final_data/commongen/commongen.test.tgt.txt
/home/bill/CommonGen-plus/methods/const-levt/constrained-levt/final.leven.alpha.test.txt
MLM-Scores
Evaluating with bert-base-en-uncased
score with bert-base-en-uncased is -47.777675966039226
Evaluating with roberta-base-en-cased
score with roberta-base-en-cased is -46.54176926830296
Evaluating with gpt2-345m-en-cased
score with gpt2-345m-en-cased is -59.55468603555275
